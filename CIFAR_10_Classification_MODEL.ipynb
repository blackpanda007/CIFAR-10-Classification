{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H1u3XYoyJry",
        "outputId": "f589a043-ac01-48cc-f0bd-76e7f1b4b06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# set device to use\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# define hyperparameters\n",
        "num_epochs = 30\n",
        "batch_size = 300\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# define transform to apply to input data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# load CIFAR-10 dataset and apply transform\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,numInputs, numOutputs,inputChannel, outputchannel):\n",
        "        super(Block, self).__init__()\n",
        "        self.Linear = nn.Linear(numInputs, numOutputs)\n",
        "        self.avgpooling = nn.AvgPool2d(kernel_size=(1, 1))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.Convl1 = nn.Conv2d(inputChannel, outputchannel, kernel_size = 3, padding=1,stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Spatial Avg Pool\n",
        "        pooling = self.avgpooling(x)\n",
        "\n",
        "        # Flattning\n",
        "        flatten_data = pooling.view(pooling.size(0), -1)\n",
        "\n",
        "        linearOut = self.Linear(flatten_data)\n",
        "\n",
        "        # RELU\n",
        "        linearRelU = self.relu(linearOut)\n",
        "\n",
        "        # ConvLayer * a1\n",
        "        co = self.Convl1(x)\n",
        "\n",
        "        mul=0\n",
        "        for col_idx in range(linearRelU.shape[1]):\n",
        "          a = linearRelU[:, col_idx]  # pick the column\n",
        "          mul += a.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)*co\n",
        "        x = mul\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "13GBrwzdyR_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Backbone(nn.Module):\n",
        "    def __init__(self, num_blocks, numInputs, numOutputs):\n",
        "        super(Backbone, self).__init__()\n",
        "        self.num_blocks = num_blocks\n",
        "        inputChannel = 3\n",
        "        outputchannel = 5\n",
        "        for i in range(num_blocks):\n",
        "          self.add_module('bk{0}'.format(i), Block(numInputs, numOutputs, inputChannel, outputchannel))\n",
        "          numInputs = outputchannel * 32 *32\n",
        "          inputChannel = outputchannel\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(inputChannel, 10))\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = x\n",
        "      for i in range(self.num_blocks):\n",
        "        out = self._modules['bk{0}'.format(i)](out)\n",
        "      x = self.classifier(out)\n",
        "      return x"
      ],
      "metadata": {
        "id": "7uCvLgPT0wmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super().__init__()\n",
        "    self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tZnEpyzA0_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Model(nn.Module):\n",
        "    def __init__(self, num_blocks,numInputs, numOutputs):\n",
        "        super().__init__()\n",
        "        self.backbone = Backbone(num_blocks,numInputs, numOutputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "W090EOGG1JiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numInputs, numOutputs = 3072, 10\n",
        "\n",
        "num_blocks=3\n",
        "\n",
        "model = CIFAR10Model(num_blocks,numInputs,numOutputs).to(device)\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "M8pvCWfH1P5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "total_step = len(trainloader)\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Track the loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}]\")\n",
        "\n",
        "    # Track the training loss and accuracy for this epoch\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in testloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Track the validation loss and accuracy for this epoch\n",
        "        val_loss = running_loss / len(testloader)\n",
        "        val_acc = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLNL0d-C3HL9",
        "outputId": "7a7af9b8-e7ba-4953-b7c8-505585d442c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [100/167]\n",
            "Epoch [1/30], Training Loss: 2.3480, Training Accuracy: 0.0997, Validation Loss: 2.3381, Validation Accuracy: 0.0994\n",
            "Epoch [2/30], Step [100/167]\n",
            "Epoch [2/30], Training Loss: 2.3395, Training Accuracy: 0.0999, Validation Loss: 2.3368, Validation Accuracy: 0.0995\n",
            "Epoch [3/30], Step [100/167]\n",
            "Epoch [3/30], Training Loss: 2.3379, Training Accuracy: 0.0999, Validation Loss: 2.3361, Validation Accuracy: 0.0997\n",
            "Epoch [4/30], Step [100/167]\n",
            "Epoch [4/30], Training Loss: 2.3370, Training Accuracy: 0.0999, Validation Loss: 2.3356, Validation Accuracy: 0.0999\n",
            "Epoch [5/30], Step [100/167]\n",
            "Epoch [5/30], Training Loss: 2.3363, Training Accuracy: 0.1000, Validation Loss: 2.3352, Validation Accuracy: 0.0999\n",
            "Epoch [6/30], Step [100/167]\n",
            "Epoch [6/30], Training Loss: 2.3357, Training Accuracy: 0.1000, Validation Loss: 2.3347, Validation Accuracy: 0.0999\n",
            "Epoch [7/30], Step [100/167]\n",
            "Epoch [7/30], Training Loss: 2.3351, Training Accuracy: 0.1001, Validation Loss: 2.3342, Validation Accuracy: 0.1001\n",
            "Epoch [8/30], Step [100/167]\n",
            "Epoch [8/30], Training Loss: 2.3346, Training Accuracy: 0.1001, Validation Loss: 2.3336, Validation Accuracy: 0.1002\n",
            "Epoch [9/30], Step [100/167]\n",
            "Epoch [9/30], Training Loss: 2.3339, Training Accuracy: 0.1003, Validation Loss: 2.3329, Validation Accuracy: 0.1003\n",
            "Epoch [10/30], Step [100/167]\n",
            "Epoch [10/30], Training Loss: 2.3332, Training Accuracy: 0.1004, Validation Loss: 2.3320, Validation Accuracy: 0.1009\n",
            "Epoch [11/30], Step [100/167]\n",
            "Epoch [11/30], Training Loss: 2.3323, Training Accuracy: 0.1007, Validation Loss: 2.3312, Validation Accuracy: 0.1012\n",
            "Epoch [12/30], Step [100/167]\n",
            "Epoch [12/30], Training Loss: 2.3315, Training Accuracy: 0.1009, Validation Loss: 2.3297, Validation Accuracy: 0.1025\n",
            "Epoch [13/30], Step [100/167]\n",
            "Epoch [13/30], Training Loss: 2.3304, Training Accuracy: 0.1015, Validation Loss: 2.3288, Validation Accuracy: 0.1026\n",
            "Epoch [14/30], Step [100/167]\n",
            "Epoch [14/30], Training Loss: 2.3296, Training Accuracy: 0.1021, Validation Loss: 2.3278, Validation Accuracy: 0.1033\n",
            "Epoch [15/30], Step [100/167]\n",
            "Epoch [15/30], Training Loss: 2.3283, Training Accuracy: 0.1028, Validation Loss: 2.3263, Validation Accuracy: 0.1045\n",
            "Epoch [16/30], Step [100/167]\n",
            "Epoch [16/30], Training Loss: 2.3274, Training Accuracy: 0.1037, Validation Loss: 2.3250, Validation Accuracy: 0.1051\n",
            "Epoch [17/30], Step [100/167]\n",
            "Epoch [17/30], Training Loss: 2.3261, Training Accuracy: 0.1045, Validation Loss: 2.3234, Validation Accuracy: 0.1070\n",
            "Epoch [18/30], Step [100/167]\n",
            "Epoch [18/30], Training Loss: 2.3247, Training Accuracy: 0.1059, Validation Loss: 2.3229, Validation Accuracy: 0.1061\n",
            "Epoch [19/30], Step [100/167]\n",
            "Epoch [19/30], Training Loss: 2.3233, Training Accuracy: 0.1064, Validation Loss: 2.3214, Validation Accuracy: 0.1074\n",
            "Epoch [20/30], Step [100/167]\n",
            "Epoch [20/30], Training Loss: 2.3215, Training Accuracy: 0.1076, Validation Loss: 2.3203, Validation Accuracy: 0.1080\n",
            "Epoch [21/30], Step [100/167]\n",
            "Epoch [21/30], Training Loss: 2.3201, Training Accuracy: 0.1085, Validation Loss: 2.3197, Validation Accuracy: 0.1083\n",
            "Epoch [22/30], Step [100/167]\n",
            "Epoch [22/30], Training Loss: 2.3186, Training Accuracy: 0.1090, Validation Loss: 2.3193, Validation Accuracy: 0.1092\n",
            "Epoch [23/30], Step [100/167]\n",
            "Epoch [23/30], Training Loss: 2.3174, Training Accuracy: 0.1101, Validation Loss: 2.3191, Validation Accuracy: 0.1092\n",
            "Epoch [24/30], Step [100/167]\n",
            "Epoch [24/30], Training Loss: 2.3162, Training Accuracy: 0.1105, Validation Loss: 2.3188, Validation Accuracy: 0.1092\n",
            "Epoch [25/30], Step [100/167]\n",
            "Epoch [25/30], Training Loss: 2.3151, Training Accuracy: 0.1113, Validation Loss: 2.3184, Validation Accuracy: 0.1099\n",
            "Epoch [26/30], Step [100/167]\n",
            "Epoch [26/30], Training Loss: 2.3141, Training Accuracy: 0.1115, Validation Loss: 2.3183, Validation Accuracy: 0.1120\n",
            "Epoch [27/30], Step [100/167]\n",
            "Epoch [27/30], Training Loss: 2.3131, Training Accuracy: 0.1120, Validation Loss: 2.3178, Validation Accuracy: 0.1121\n",
            "Epoch [28/30], Step [100/167]\n",
            "Epoch [28/30], Training Loss: 2.3119, Training Accuracy: 0.1127, Validation Loss: 2.3174, Validation Accuracy: 0.1123\n",
            "Epoch [29/30], Step [100/167]\n",
            "Epoch [29/30], Training Loss: 2.3108, Training Accuracy: 0.1129, Validation Loss: 2.3169, Validation Accuracy: 0.1123\n",
            "Epoch [30/30], Step [100/167]\n",
            "Epoch [30/30], Training Loss: 2.3098, Training Accuracy: 0.1134, Validation Loss: 2.3163, Validation Accuracy: 0.1124\n"
          ]
        }
      ]
    }
  ]
}